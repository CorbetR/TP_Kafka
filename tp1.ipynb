{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions d'Installation et de Configuration de l'Environnement pour le Projet Kafka\n",
    "\n",
    "Ce document fournit un guide pour installer Python 3.10, configurer un environnement virtuel, et installer les bibliothèques nécessaires pour le projet de streaming de données avec Kafka.\n",
    "\n",
    "---\n",
    "\n",
    "## Installation de Python 3.10\n",
    "\n",
    "1. **Ajouter le dépôt de Python** :\n",
    "\n",
    "   ```bash\n",
    "   sudo apt update\n",
    "   sudo apt install -y software-properties-common\n",
    "   sudo add-apt-repository ppa:deadsnakes/ppa\n",
    "   sudo apt update\n",
    "\n",
    "\n",
    "2. **Installer Python 3.10 :** :\n",
    "\n",
    "   ```bash\n",
    "   sudo apt install python3.10\n",
    "\n",
    "3. **Installer le module venv pour Python 3.10 :** :\n",
    "\n",
    "   ```bash\n",
    "   sudo apt install python3.10-venv\n",
    "\n",
    "\n",
    "## Création et Activation d'un Environnement Virtuel\n",
    "\n",
    "4. **Créer l’environnement virtuel :** :\n",
    "\n",
    "   ```bash\n",
    "   python3.10 -m venv kafka_env\n",
    "\n",
    "5. **Activer l’environnement virtuel :** :\n",
    "\n",
    "   ```bash\n",
    "   source kafka_env/bin/activate\n",
    "\n",
    "## Installation des Bibliothèques Requises\n",
    "Dans l’environnement virtuel, installez les bibliothèques suivantes :\n",
    "\n",
    "5. **kafka-python : pour interagir avec Kafka en tant que producteur et consumer.** :\n",
    "\n",
    "   ```bash\n",
    "   pip install kafka-python\n",
    "5. **requests : pour récupérer les données depuis l'API** :\n",
    "\n",
    "   ```bash\n",
    "   pip install requests\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Configuration\n",
    "API_KEY = '5cd0c68e3773c0513b41138ade44163b'  # Clé API\n",
    "CITIES = ['Paris']  # Villes cibles\n",
    "\n",
    "\n",
    "# Fonction pour récupérer les données météo\n",
    "def get_weather_data(city):\n",
    "    url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={API_KEY}&units=metric\"\n",
    "    response = requests.get(url)\n",
    "    return response.json() if response.status_code == 200 else None\n",
    "\n",
    "get_weather_data(\"Lyon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Zookeeper...\n",
      "[2024-11-14 16:38:49,013] INFO Reading configuration from: ./kafka_2.12-2.6.0/config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2024-11-14 16:38:49,017] INFO clientPortAddress is 0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2024-11-14 16:38:49,017] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2024-11-14 16:38:49,019] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)\n",
      "[2024-11-14 16:38:49,020] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)\n",
      "[2024-11-14 16:38:49,020] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)\n",
      "[2024-11-14 16:38:49,021] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)\n",
      "[2024-11-14 16:38:49,026] INFO Log4j 1.2 jmx support found and enabled. (org.apache.zookeeper.jmx.ManagedUtil)\n",
      "[2024-11-14 16:38:49,071] INFO Reading configuration from: ./kafka_2.12-2.6.0/config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2024-11-14 16:38:49,072] INFO clientPortAddress is 0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2024-11-14 16:38:49,072] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2024-11-14 16:38:49,073] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)\n",
      "[2024-11-14 16:38:49,077] INFO zookeeper.snapshot.trust.empty : false (org.apache.zookeeper.server.persistence.FileTxnSnapLog)\n",
      "[2024-11-14 16:38:49,105] INFO Server environment:zookeeper.version=3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:53 GMT (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2024-11-14 16:38:49,105] INFO Server environment:host.name=localhost (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2024-11-14 16:38:49,105] INFO Server environment:java.version=21.0.4 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2024-11-14 16:38:49,105] INFO Server environment:java.vendor=Microsoft (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2024-11-14 16:38:49,105] INFO Server environment:java.home=/usr/local/sdkman/candidates/java/21.0.4-ms (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2024-11-14 16:38:49,105] INFO Server environment:java.class.path=/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/activation-1.1.1.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/aopalliance-repackaged-2.5.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/argparse4j-0.7.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/audience-annotations-0.5.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/commons-cli-1.4.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/commons-lang3-3.8.1.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/connect-api-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/connect-basic-auth-extension-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/connect-file-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/connect-json-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/connect-mirror-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/connect-mirror-client-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/connect-runtime-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/connect-transforms-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/hk2-api-2.5.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/hk2-locator-2.5.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/hk2-utils-2.5.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jackson-annotations-2.10.2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jackson-core-2.10.2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jackson-databind-2.10.2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jackson-dataformat-csv-2.10.2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jackson-datatype-jdk8-2.10.2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jackson-jaxrs-base-2.10.2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jackson-module-paranamer-2.10.2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jackson-module-scala_2.12-2.10.2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jakarta.activation-api-1.2.1.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jakarta.annotation-api-1.3.4.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jakarta.inject-2.5.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jakarta.ws.rs-api-2.1.5.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jakarta.xml.bind-api-2.3.2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/javassist-3.22.0-CR2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/javassist-3.26.0-GA.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/javax.servlet-api-3.1.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jaxb-api-2.3.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jersey-client-2.28.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jersey-common-2.28.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jersey-container-servlet-2.28.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jersey-container-servlet-core-2.28.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jersey-hk2-2.28.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jersey-media-jaxb-2.28.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jersey-server-2.28.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jetty-client-9.4.24.v20191120.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jetty-continuation-9.4.24.v20191120.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jetty-http-9.4.24.v20191120.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jetty-io-9.4.24.v20191120.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jetty-security-9.4.24.v20191120.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jetty-server-9.4.24.v20191120.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jetty-servlet-9.4.24.v20191120.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jetty-servlets-9.4.24.v20191120.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jetty-util-9.4.24.v20191120.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jopt-simple-5.0.4.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/kafka-clients-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/kafka-log4j-appender-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/kafka-streams-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/kafka-streams-examples-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/kafka-streams-scala_2.12-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/kafka-streams-test-utils-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/kafka-tools-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/kafka_2.12-2.6.0-sources.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/kafka_2.12-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/log4j-1.2.17.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/lz4-java-1.7.1.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/maven-artifact-3.6.3.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/metrics-core-2.2.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/netty-buffer-4.1.50.Final.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/netty-codec-4.1.50.Final.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/netty-common-4.1.50.Final.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/netty-handler-4.1.50.Final.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/netty-resolver-4.1.50.Final.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/netty-transport-4.1.50.Final.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/netty-transport-native-epoll-4.1.50.Final.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/netty-transport-native-unix-common-4.1.50.Final.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/osgi-resource-locator-1.0.1.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/paranamer-2.8.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/plexus-utils-3.2.1.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/reflections-0.9.12.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/rocksdbjni-5.18.4.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/scala-collection-compat_2.12-2.1.6.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/scala-java8-compat_2.12-0.9.1.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/scala-library-2.12.11.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/scala-logging_2.12-3.9.2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/scala-reflect-2.12.11.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/slf4j-api-1.7.30.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/slf4j-log4j12-1.7.30.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/snappy-java-1.1.7.3.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/validation-api-2.0.1.Final.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/zookeeper-3.5.8.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/zookeeper-jute-3.5.8.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/zstd-jni-1.4.4-7.jar (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2024-11-14 16:38:49,106] INFO Server environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2024-11-14 16:38:49,106] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2024-11-14 16:38:49,106] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2024-11-14 16:38:49,106] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2024-11-14 16:38:49,106] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2024-11-14 16:38:49,106] INFO Server environment:os.version=6.5.0-1025-azure (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2024-11-14 16:38:49,106] INFO Server environment:user.name=codespace (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2024-11-14 16:38:49,106] INFO Server environment:user.home=/home/codespace (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2024-11-14 16:38:49,106] INFO Server environment:user.dir=/workspaces/TP_Kafka (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2024-11-14 16:38:49,106] INFO Server environment:os.memory.free=494MB (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2024-11-14 16:38:49,106] INFO Server environment:os.memory.max=512MB (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2024-11-14 16:38:49,107] INFO Server environment:os.memory.total=512MB (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2024-11-14 16:38:49,113] INFO minSessionTimeout set to 6000 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2024-11-14 16:38:49,116] INFO maxSessionTimeout set to 60000 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2024-11-14 16:38:49,116] INFO Created server with tickTime 3000 minSessionTimeout 6000 maxSessionTimeout 60000 datadir /tmp/zookeeper/version-2 snapdir /tmp/zookeeper/version-2 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2024-11-14 16:38:49,127] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)\n",
      "[2024-11-14 16:38:49,132] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 1 selector thread(s), 4 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory)\n",
      "[2024-11-14 16:38:49,139] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)\n",
      "[2024-11-14 16:38:49,151] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase)\n",
      "[2024-11-14 16:38:49,155] INFO Snapshotting: 0x0 to /tmp/zookeeper/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)\n",
      "[2024-11-14 16:38:49,158] INFO Snapshotting: 0x0 to /tmp/zookeeper/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)\n",
      "[2024-11-14 16:38:49,176] INFO Using checkIntervalMs=60000 maxPerMinute=10000 (org.apache.zookeeper.server.ContainerManager)\n",
      "Starting Kafka...\n",
      "[2024-11-14 16:38:53,865] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)\n",
      "[2024-11-14 16:38:54,331] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)\n",
      "[2024-11-14 16:38:54,418] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)\n",
      "[2024-11-14 16:38:54,421] INFO starting (kafka.server.KafkaServer)\n",
      "[2024-11-14 16:38:54,422] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)\n",
      "[2024-11-14 16:38:54,450] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)\n",
      "[2024-11-14 16:38:54,454] INFO Client environment:zookeeper.version=3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:53 GMT (org.apache.zookeeper.ZooKeeper)\n",
      "[2024-11-14 16:38:54,454] INFO Client environment:host.name=localhost (org.apache.zookeeper.ZooKeeper)\n",
      "[2024-11-14 16:38:54,454] INFO Client environment:java.version=21.0.4 (org.apache.zookeeper.ZooKeeper)\n",
      "[2024-11-14 16:38:54,454] INFO Client environment:java.vendor=Microsoft (org.apache.zookeeper.ZooKeeper)\n",
      "[2024-11-14 16:38:54,454] INFO Client environment:java.home=/usr/local/sdkman/candidates/java/21.0.4-ms (org.apache.zookeeper.ZooKeeper)\n",
      "[2024-11-14 16:38:54,455] INFO Client environment:java.class.path=/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/activation-1.1.1.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/aopalliance-repackaged-2.5.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/argparse4j-0.7.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/audience-annotations-0.5.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/commons-cli-1.4.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/commons-lang3-3.8.1.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/connect-api-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/connect-basic-auth-extension-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/connect-file-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/connect-json-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/connect-mirror-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/connect-mirror-client-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/connect-runtime-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/connect-transforms-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/hk2-api-2.5.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/hk2-locator-2.5.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/hk2-utils-2.5.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jackson-annotations-2.10.2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jackson-core-2.10.2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jackson-databind-2.10.2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jackson-dataformat-csv-2.10.2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jackson-datatype-jdk8-2.10.2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jackson-jaxrs-base-2.10.2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jackson-jaxrs-json-provider-2.10.2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jackson-module-jaxb-annotations-2.10.2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jackson-module-paranamer-2.10.2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jackson-module-scala_2.12-2.10.2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jakarta.activation-api-1.2.1.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jakarta.annotation-api-1.3.4.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jakarta.inject-2.5.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jakarta.ws.rs-api-2.1.5.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jakarta.xml.bind-api-2.3.2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/javassist-3.22.0-CR2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/javassist-3.26.0-GA.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/javax.servlet-api-3.1.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jaxb-api-2.3.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jersey-client-2.28.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jersey-common-2.28.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jersey-container-servlet-2.28.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jersey-container-servlet-core-2.28.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jersey-hk2-2.28.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jersey-media-jaxb-2.28.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jersey-server-2.28.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jetty-client-9.4.24.v20191120.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jetty-continuation-9.4.24.v20191120.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jetty-http-9.4.24.v20191120.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jetty-io-9.4.24.v20191120.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jetty-security-9.4.24.v20191120.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jetty-server-9.4.24.v20191120.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jetty-servlet-9.4.24.v20191120.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jetty-servlets-9.4.24.v20191120.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jetty-util-9.4.24.v20191120.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/jopt-simple-5.0.4.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/kafka-clients-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/kafka-log4j-appender-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/kafka-streams-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/kafka-streams-examples-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/kafka-streams-scala_2.12-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/kafka-streams-test-utils-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/kafka-tools-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/kafka_2.12-2.6.0-sources.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/kafka_2.12-2.6.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/log4j-1.2.17.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/lz4-java-1.7.1.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/maven-artifact-3.6.3.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/metrics-core-2.2.0.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/netty-buffer-4.1.50.Final.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/netty-codec-4.1.50.Final.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/netty-common-4.1.50.Final.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/netty-handler-4.1.50.Final.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/netty-resolver-4.1.50.Final.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/netty-transport-4.1.50.Final.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/netty-transport-native-epoll-4.1.50.Final.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/netty-transport-native-unix-common-4.1.50.Final.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/osgi-resource-locator-1.0.1.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/paranamer-2.8.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/plexus-utils-3.2.1.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/reflections-0.9.12.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/rocksdbjni-5.18.4.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/scala-collection-compat_2.12-2.1.6.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/scala-java8-compat_2.12-0.9.1.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/scala-library-2.12.11.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/scala-logging_2.12-3.9.2.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/scala-reflect-2.12.11.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/slf4j-api-1.7.30.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/slf4j-log4j12-1.7.30.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/snappy-java-1.1.7.3.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/validation-api-2.0.1.Final.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/zookeeper-3.5.8.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/zookeeper-jute-3.5.8.jar:/workspaces/TP_Kafka/kafka_2.12-2.6.0/bin/../libs/zstd-jni-1.4.4-7.jar (org.apache.zookeeper.ZooKeeper)\n",
      "[2024-11-14 16:38:54,456] INFO Client environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)\n",
      "[2024-11-14 16:38:54,456] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)\n",
      "[2024-11-14 16:38:54,456] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)\n",
      "[2024-11-14 16:38:54,457] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)\n",
      "[2024-11-14 16:38:54,457] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)\n",
      "[2024-11-14 16:38:54,457] INFO Client environment:os.version=6.5.0-1025-azure (org.apache.zookeeper.ZooKeeper)\n",
      "[2024-11-14 16:38:54,457] INFO Client environment:user.name=codespace (org.apache.zookeeper.ZooKeeper)\n",
      "[2024-11-14 16:38:54,458] INFO Client environment:user.home=/home/codespace (org.apache.zookeeper.ZooKeeper)\n",
      "[2024-11-14 16:38:54,458] INFO Client environment:user.dir=/workspaces/TP_Kafka (org.apache.zookeeper.ZooKeeper)\n",
      "[2024-11-14 16:38:54,458] INFO Client environment:os.memory.free=989MB (org.apache.zookeeper.ZooKeeper)\n",
      "[2024-11-14 16:38:54,458] INFO Client environment:os.memory.max=1024MB (org.apache.zookeeper.ZooKeeper)\n",
      "[2024-11-14 16:38:54,458] INFO Client environment:os.memory.total=1024MB (org.apache.zookeeper.ZooKeeper)\n",
      "[2024-11-14 16:38:54,461] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@75c56eb9 (org.apache.zookeeper.ZooKeeper)\n",
      "[2024-11-14 16:38:54,471] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)\n",
      "[2024-11-14 16:38:54,481] INFO zookeeper.request.timeout value is 0. feature enabled= (org.apache.zookeeper.ClientCnxn)\n",
      "[2024-11-14 16:38:54,497] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)\n",
      "[2024-11-14 16:38:54,499] INFO Opening socket connection to server localhost/[0:0:0:0:0:0:0:1]:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)\n",
      "[2024-11-14 16:38:54,522] INFO Socket connection established, initiating session, client: /[0:0:0:0:0:0:0:1]:56290, server: localhost/[0:0:0:0:0:0:0:1]:2181 (org.apache.zookeeper.ClientCnxn)\n",
      "[2024-11-14 16:38:54,534] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)\n",
      "[2024-11-14 16:38:54,554] INFO Session establishment complete on server localhost/[0:0:0:0:0:0:0:1]:2181, sessionid = 0x100000534c60000, negotiated timeout = 18000 (org.apache.zookeeper.ClientCnxn)\n",
      "[2024-11-14 16:38:54,559] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)\n",
      "[2024-11-14 16:38:54,881] WARN Exception causing close of session 0x0: Len error 1195725856 (org.apache.zookeeper.server.NIOServerCnxn)\n",
      "[2024-11-14 16:38:54,909] WARN Exception causing close of session 0x0: Len error 1195725856 (org.apache.zookeeper.server.NIOServerCnxn)\n",
      "[2024-11-14 16:38:54,945] WARN Exception causing close of session 0x0: Len error 1195725856 (org.apache.zookeeper.server.NIOServerCnxn)\n",
      "[2024-11-14 16:38:54,963] WARN Exception causing close of session 0x0: Len error 1195725856 (org.apache.zookeeper.server.NIOServerCnxn)\n",
      "[2024-11-14 16:38:55,026] INFO Cluster ID = z093lWzGRI6JdmfaMPGsMw (kafka.server.KafkaServer)\n",
      "[2024-11-14 16:38:55,033] WARN No meta.properties file under dir /tmp/kafka-logs/meta.properties (kafka.server.BrokerMetadataCheckpoint)\n",
      "[2024-11-14 16:38:55,135] INFO KafkaConfig values: \n",
      "\tadvertised.host.name = null\n",
      "\tadvertised.listeners = null\n",
      "\tadvertised.port = null\n",
      "\talter.config.policy.class.name = null\n",
      "\talter.log.dirs.replication.quota.window.num = 11\n",
      "\talter.log.dirs.replication.quota.window.size.seconds = 1\n",
      "\tauthorizer.class.name = \n",
      "\tauto.create.topics.enable = true\n",
      "\tauto.leader.rebalance.enable = true\n",
      "\tbackground.threads = 10\n",
      "\tbroker.id = 0\n",
      "\tbroker.id.generation.enable = true\n",
      "\tbroker.rack = null\n",
      "\tclient.quota.callback.class = null\n",
      "\tcompression.type = producer\n",
      "\tconnection.failed.authentication.delay.ms = 100\n",
      "\tconnections.max.idle.ms = 600000\n",
      "\tconnections.max.reauth.ms = 0\n",
      "\tcontrol.plane.listener.name = null\n",
      "\tcontrolled.shutdown.enable = true\n",
      "\tcontrolled.shutdown.max.retries = 3\n",
      "\tcontrolled.shutdown.retry.backoff.ms = 5000\n",
      "\tcontroller.socket.timeout.ms = 30000\n",
      "\tcreate.topic.policy.class.name = null\n",
      "\tdefault.replication.factor = 1\n",
      "\tdelegation.token.expiry.check.interval.ms = 3600000\n",
      "\tdelegation.token.expiry.time.ms = 86400000\n",
      "\tdelegation.token.master.key = null\n",
      "\tdelegation.token.max.lifetime.ms = 604800000\n",
      "\tdelete.records.purgatory.purge.interval.requests = 1\n",
      "\tdelete.topic.enable = true\n",
      "\tfetch.max.bytes = 57671680\n",
      "\tfetch.purgatory.purge.interval.requests = 1000\n",
      "\tgroup.initial.rebalance.delay.ms = 0\n",
      "\tgroup.max.session.timeout.ms = 1800000\n",
      "\tgroup.max.size = 2147483647\n",
      "\tgroup.min.session.timeout.ms = 6000\n",
      "\thost.name = \n",
      "\tinter.broker.listener.name = null\n",
      "\tinter.broker.protocol.version = 2.6-IV0\n",
      "\tkafka.metrics.polling.interval.secs = 10\n",
      "\tkafka.metrics.reporters = []\n",
      "\tleader.imbalance.check.interval.seconds = 300\n",
      "\tleader.imbalance.per.broker.percentage = 10\n",
      "\tlistener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL\n",
      "\tlisteners = null\n",
      "\tlog.cleaner.backoff.ms = 15000\n",
      "\tlog.cleaner.dedupe.buffer.size = 134217728\n",
      "\tlog.cleaner.delete.retention.ms = 86400000\n",
      "\tlog.cleaner.enable = true\n",
      "\tlog.cleaner.io.buffer.load.factor = 0.9\n",
      "\tlog.cleaner.io.buffer.size = 524288\n",
      "\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\n",
      "\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\n",
      "\tlog.cleaner.min.cleanable.ratio = 0.5\n",
      "\tlog.cleaner.min.compaction.lag.ms = 0\n",
      "\tlog.cleaner.threads = 1\n",
      "\tlog.cleanup.policy = [delete]\n",
      "\tlog.dir = /tmp/kafka-logs\n",
      "\tlog.dirs = /tmp/kafka-logs\n",
      "\tlog.flush.interval.messages = 9223372036854775807\n",
      "\tlog.flush.interval.ms = null\n",
      "\tlog.flush.offset.checkpoint.interval.ms = 60000\n",
      "\tlog.flush.scheduler.interval.ms = 9223372036854775807\n",
      "\tlog.flush.start.offset.checkpoint.interval.ms = 60000\n",
      "\tlog.index.interval.bytes = 4096\n",
      "\tlog.index.size.max.bytes = 10485760\n",
      "\tlog.message.downconversion.enable = true\n",
      "\tlog.message.format.version = 2.6-IV0\n",
      "\tlog.message.timestamp.difference.max.ms = 9223372036854775807\n",
      "\tlog.message.timestamp.type = CreateTime\n",
      "\tlog.preallocate = false\n",
      "\tlog.retention.bytes = -1\n",
      "\tlog.retention.check.interval.ms = 300000\n",
      "\tlog.retention.hours = 168\n",
      "\tlog.retention.minutes = null\n",
      "\tlog.retention.ms = null\n",
      "\tlog.roll.hours = 168\n",
      "\tlog.roll.jitter.hours = 0\n",
      "\tlog.roll.jitter.ms = null\n",
      "\tlog.roll.ms = null\n",
      "\tlog.segment.bytes = 1073741824\n",
      "\tlog.segment.delete.delay.ms = 60000\n",
      "\tmax.connections = 2147483647\n",
      "\tmax.connections.per.ip = 2147483647\n",
      "\tmax.connections.per.ip.overrides = \n",
      "\tmax.incremental.fetch.session.cache.slots = 1000\n",
      "\tmessage.max.bytes = 1048588\n",
      "\tmetric.reporters = []\n",
      "\tmetrics.num.samples = 2\n",
      "\tmetrics.recording.level = INFO\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\tmin.insync.replicas = 1\n",
      "\tnum.io.threads = 8\n",
      "\tnum.network.threads = 3\n",
      "\tnum.partitions = 1\n",
      "\tnum.recovery.threads.per.data.dir = 1\n",
      "\tnum.replica.alter.log.dirs.threads = null\n",
      "\tnum.replica.fetchers = 1\n",
      "\toffset.metadata.max.bytes = 4096\n",
      "\toffsets.commit.required.acks = -1\n",
      "\toffsets.commit.timeout.ms = 5000\n",
      "\toffsets.load.buffer.size = 5242880\n",
      "\toffsets.retention.check.interval.ms = 600000\n",
      "\toffsets.retention.minutes = 10080\n",
      "\toffsets.topic.compression.codec = 0\n",
      "\toffsets.topic.num.partitions = 50\n",
      "\toffsets.topic.replication.factor = 1\n",
      "\toffsets.topic.segment.bytes = 104857600\n",
      "\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\n",
      "\tpassword.encoder.iterations = 4096\n",
      "\tpassword.encoder.key.length = 128\n",
      "\tpassword.encoder.keyfactory.algorithm = null\n",
      "\tpassword.encoder.old.secret = null\n",
      "\tpassword.encoder.secret = null\n",
      "\tport = 9092\n",
      "\tprincipal.builder.class = null\n",
      "\tproducer.purgatory.purge.interval.requests = 1000\n",
      "\tqueued.max.request.bytes = -1\n",
      "\tqueued.max.requests = 500\n",
      "\tquota.consumer.default = 9223372036854775807\n",
      "\tquota.producer.default = 9223372036854775807\n",
      "\tquota.window.num = 11\n",
      "\tquota.window.size.seconds = 1\n",
      "\treplica.fetch.backoff.ms = 1000\n",
      "\treplica.fetch.max.bytes = 1048576\n",
      "\treplica.fetch.min.bytes = 1\n",
      "\treplica.fetch.response.max.bytes = 10485760\n",
      "\treplica.fetch.wait.max.ms = 500\n",
      "\treplica.high.watermark.checkpoint.interval.ms = 5000\n",
      "\treplica.lag.time.max.ms = 30000\n",
      "\treplica.selector.class = null\n",
      "\treplica.socket.receive.buffer.bytes = 65536\n",
      "\treplica.socket.timeout.ms = 30000\n",
      "\treplication.quota.window.num = 11\n",
      "\treplication.quota.window.size.seconds = 1\n",
      "\trequest.timeout.ms = 30000\n",
      "\treserved.broker.max.id = 1000\n",
      "\tsasl.client.callback.handler.class = null\n",
      "\tsasl.enabled.mechanisms = [GSSAPI]\n",
      "\tsasl.jaas.config = null\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tsasl.login.callback.handler.class = null\n",
      "\tsasl.login.class = null\n",
      "\tsasl.login.refresh.buffer.seconds = 300\n",
      "\tsasl.login.refresh.min.period.seconds = 60\n",
      "\tsasl.login.refresh.window.factor = 0.8\n",
      "\tsasl.login.refresh.window.jitter = 0.05\n",
      "\tsasl.mechanism.inter.broker.protocol = GSSAPI\n",
      "\tsasl.server.callback.handler.class = null\n",
      "\tsecurity.inter.broker.protocol = PLAINTEXT\n",
      "\tsecurity.providers = null\n",
      "\tsocket.receive.buffer.bytes = 102400\n",
      "\tsocket.request.max.bytes = 104857600\n",
      "\tsocket.send.buffer.bytes = 102400\n",
      "\tssl.cipher.suites = []\n",
      "\tssl.client.auth = none\n",
      "\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n",
      "\tssl.endpoint.identification.algorithm = https\n",
      "\tssl.engine.factory.class = null\n",
      "\tssl.key.password = null\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tssl.keystore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tssl.keystore.type = JKS\n",
      "\tssl.principal.mapping.rules = DEFAULT\n",
      "\tssl.protocol = TLSv1.3\n",
      "\tssl.provider = null\n",
      "\tssl.secure.random.implementation = null\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.truststore.location = null\n",
      "\tssl.truststore.password = null\n",
      "\tssl.truststore.type = JKS\n",
      "\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\n",
      "\ttransaction.max.timeout.ms = 900000\n",
      "\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\n",
      "\ttransaction.state.log.load.buffer.size = 5242880\n",
      "\ttransaction.state.log.min.isr = 1\n",
      "\ttransaction.state.log.num.partitions = 50\n",
      "\ttransaction.state.log.replication.factor = 1\n",
      "\ttransaction.state.log.segment.bytes = 104857600\n",
      "\ttransactional.id.expiration.ms = 604800000\n",
      "\tunclean.leader.election.enable = false\n",
      "\tzookeeper.clientCnxnSocket = null\n",
      "\tzookeeper.connect = localhost:2181\n",
      "\tzookeeper.connection.timeout.ms = 18000\n",
      "\tzookeeper.max.in.flight.requests = 10\n",
      "\tzookeeper.session.timeout.ms = 18000\n",
      "\tzookeeper.set.acl = false\n",
      "\tzookeeper.ssl.cipher.suites = null\n",
      "\tzookeeper.ssl.client.enable = false\n",
      "\tzookeeper.ssl.crl.enable = false\n",
      "\tzookeeper.ssl.enabled.protocols = null\n",
      "\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\n",
      "\tzookeeper.ssl.keystore.location = null\n",
      "\tzookeeper.ssl.keystore.password = null\n",
      "\tzookeeper.ssl.keystore.type = null\n",
      "\tzookeeper.ssl.ocsp.enable = false\n",
      "\tzookeeper.ssl.protocol = TLSv1.2\n",
      "\tzookeeper.ssl.truststore.location = null\n",
      "\tzookeeper.ssl.truststore.password = null\n",
      "\tzookeeper.ssl.truststore.type = null\n",
      "\tzookeeper.sync.time.ms = 2000\n",
      " (kafka.server.KafkaConfig)\n",
      "[2024-11-14 16:38:55,155] INFO KafkaConfig values: \n",
      "\tadvertised.host.name = null\n",
      "\tadvertised.listeners = null\n",
      "\tadvertised.port = null\n",
      "\talter.config.policy.class.name = null\n",
      "\talter.log.dirs.replication.quota.window.num = 11\n",
      "\talter.log.dirs.replication.quota.window.size.seconds = 1\n",
      "\tauthorizer.class.name = \n",
      "\tauto.create.topics.enable = true\n",
      "\tauto.leader.rebalance.enable = true\n",
      "\tbackground.threads = 10\n",
      "\tbroker.id = 0\n",
      "\tbroker.id.generation.enable = true\n",
      "\tbroker.rack = null\n",
      "\tclient.quota.callback.class = null\n",
      "\tcompression.type = producer\n",
      "\tconnection.failed.authentication.delay.ms = 100\n",
      "\tconnections.max.idle.ms = 600000\n",
      "\tconnections.max.reauth.ms = 0\n",
      "\tcontrol.plane.listener.name = null\n",
      "\tcontrolled.shutdown.enable = true\n",
      "\tcontrolled.shutdown.max.retries = 3\n",
      "\tcontrolled.shutdown.retry.backoff.ms = 5000\n",
      "\tcontroller.socket.timeout.ms = 30000\n",
      "\tcreate.topic.policy.class.name = null\n",
      "\tdefault.replication.factor = 1\n",
      "\tdelegation.token.expiry.check.interval.ms = 3600000\n",
      "\tdelegation.token.expiry.time.ms = 86400000\n",
      "\tdelegation.token.master.key = null\n",
      "\tdelegation.token.max.lifetime.ms = 604800000\n",
      "\tdelete.records.purgatory.purge.interval.requests = 1\n",
      "\tdelete.topic.enable = true\n",
      "\tfetch.max.bytes = 57671680\n",
      "\tfetch.purgatory.purge.interval.requests = 1000\n",
      "\tgroup.initial.rebalance.delay.ms = 0\n",
      "\tgroup.max.session.timeout.ms = 1800000\n",
      "\tgroup.max.size = 2147483647\n",
      "\tgroup.min.session.timeout.ms = 6000\n",
      "\thost.name = \n",
      "\tinter.broker.listener.name = null\n",
      "\tinter.broker.protocol.version = 2.6-IV0\n",
      "\tkafka.metrics.polling.interval.secs = 10\n",
      "\tkafka.metrics.reporters = []\n",
      "\tleader.imbalance.check.interval.seconds = 300\n",
      "\tleader.imbalance.per.broker.percentage = 10\n",
      "\tlistener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL\n",
      "\tlisteners = null\n",
      "\tlog.cleaner.backoff.ms = 15000\n",
      "\tlog.cleaner.dedupe.buffer.size = 134217728\n",
      "\tlog.cleaner.delete.retention.ms = 86400000\n",
      "\tlog.cleaner.enable = true\n",
      "\tlog.cleaner.io.buffer.load.factor = 0.9\n",
      "\tlog.cleaner.io.buffer.size = 524288\n",
      "\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\n",
      "\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\n",
      "\tlog.cleaner.min.cleanable.ratio = 0.5\n",
      "\tlog.cleaner.min.compaction.lag.ms = 0\n",
      "\tlog.cleaner.threads = 1\n",
      "\tlog.cleanup.policy = [delete]\n",
      "\tlog.dir = /tmp/kafka-logs\n",
      "\tlog.dirs = /tmp/kafka-logs\n",
      "\tlog.flush.interval.messages = 9223372036854775807\n",
      "\tlog.flush.interval.ms = null\n",
      "\tlog.flush.offset.checkpoint.interval.ms = 60000\n",
      "\tlog.flush.scheduler.interval.ms = 9223372036854775807\n",
      "\tlog.flush.start.offset.checkpoint.interval.ms = 60000\n",
      "\tlog.index.interval.bytes = 4096\n",
      "\tlog.index.size.max.bytes = 10485760\n",
      "\tlog.message.downconversion.enable = true\n",
      "\tlog.message.format.version = 2.6-IV0\n",
      "\tlog.message.timestamp.difference.max.ms = 9223372036854775807\n",
      "\tlog.message.timestamp.type = CreateTime\n",
      "\tlog.preallocate = false\n",
      "\tlog.retention.bytes = -1\n",
      "\tlog.retention.check.interval.ms = 300000\n",
      "\tlog.retention.hours = 168\n",
      "\tlog.retention.minutes = null\n",
      "\tlog.retention.ms = null\n",
      "\tlog.roll.hours = 168\n",
      "\tlog.roll.jitter.hours = 0\n",
      "\tlog.roll.jitter.ms = null\n",
      "\tlog.roll.ms = null\n",
      "\tlog.segment.bytes = 1073741824\n",
      "\tlog.segment.delete.delay.ms = 60000\n",
      "\tmax.connections = 2147483647\n",
      "\tmax.connections.per.ip = 2147483647\n",
      "\tmax.connections.per.ip.overrides = \n",
      "\tmax.incremental.fetch.session.cache.slots = 1000\n",
      "\tmessage.max.bytes = 1048588\n",
      "\tmetric.reporters = []\n",
      "\tmetrics.num.samples = 2\n",
      "\tmetrics.recording.level = INFO\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\tmin.insync.replicas = 1\n",
      "\tnum.io.threads = 8\n",
      "\tnum.network.threads = 3\n",
      "\tnum.partitions = 1\n",
      "\tnum.recovery.threads.per.data.dir = 1\n",
      "\tnum.replica.alter.log.dirs.threads = null\n",
      "\tnum.replica.fetchers = 1\n",
      "\toffset.metadata.max.bytes = 4096\n",
      "\toffsets.commit.required.acks = -1\n",
      "\toffsets.commit.timeout.ms = 5000\n",
      "\toffsets.load.buffer.size = 5242880\n",
      "\toffsets.retention.check.interval.ms = 600000\n",
      "\toffsets.retention.minutes = 10080\n",
      "\toffsets.topic.compression.codec = 0\n",
      "\toffsets.topic.num.partitions = 50\n",
      "\toffsets.topic.replication.factor = 1\n",
      "\toffsets.topic.segment.bytes = 104857600\n",
      "\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\n",
      "\tpassword.encoder.iterations = 4096\n",
      "\tpassword.encoder.key.length = 128\n",
      "\tpassword.encoder.keyfactory.algorithm = null\n",
      "\tpassword.encoder.old.secret = null\n",
      "\tpassword.encoder.secret = null\n",
      "\tport = 9092\n",
      "\tprincipal.builder.class = null\n",
      "\tproducer.purgatory.purge.interval.requests = 1000\n",
      "\tqueued.max.request.bytes = -1\n",
      "\tqueued.max.requests = 500\n",
      "\tquota.consumer.default = 9223372036854775807\n",
      "\tquota.producer.default = 9223372036854775807\n",
      "\tquota.window.num = 11\n",
      "\tquota.window.size.seconds = 1\n",
      "\treplica.fetch.backoff.ms = 1000\n",
      "\treplica.fetch.max.bytes = 1048576\n",
      "\treplica.fetch.min.bytes = 1\n",
      "\treplica.fetch.response.max.bytes = 10485760\n",
      "\treplica.fetch.wait.max.ms = 500\n",
      "\treplica.high.watermark.checkpoint.interval.ms = 5000\n",
      "\treplica.lag.time.max.ms = 30000\n",
      "\treplica.selector.class = null\n",
      "\treplica.socket.receive.buffer.bytes = 65536\n",
      "\treplica.socket.timeout.ms = 30000\n",
      "\treplication.quota.window.num = 11\n",
      "\treplication.quota.window.size.seconds = 1\n",
      "\trequest.timeout.ms = 30000\n",
      "\treserved.broker.max.id = 1000\n",
      "\tsasl.client.callback.handler.class = null\n",
      "\tsasl.enabled.mechanisms = [GSSAPI]\n",
      "\tsasl.jaas.config = null\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tsasl.login.callback.handler.class = null\n",
      "\tsasl.login.class = null\n",
      "\tsasl.login.refresh.buffer.seconds = 300\n",
      "\tsasl.login.refresh.min.period.seconds = 60\n",
      "\tsasl.login.refresh.window.factor = 0.8\n",
      "\tsasl.login.refresh.window.jitter = 0.05\n",
      "\tsasl.mechanism.inter.broker.protocol = GSSAPI\n",
      "\tsasl.server.callback.handler.class = null\n",
      "\tsecurity.inter.broker.protocol = PLAINTEXT\n",
      "\tsecurity.providers = null\n",
      "\tsocket.receive.buffer.bytes = 102400\n",
      "\tsocket.request.max.bytes = 104857600\n",
      "\tsocket.send.buffer.bytes = 102400\n",
      "\tssl.cipher.suites = []\n",
      "\tssl.client.auth = none\n",
      "\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n",
      "\tssl.endpoint.identification.algorithm = https\n",
      "\tssl.engine.factory.class = null\n",
      "\tssl.key.password = null\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tssl.keystore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tssl.keystore.type = JKS\n",
      "\tssl.principal.mapping.rules = DEFAULT\n",
      "\tssl.protocol = TLSv1.3\n",
      "\tssl.provider = null\n",
      "\tssl.secure.random.implementation = null\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.truststore.location = null\n",
      "\tssl.truststore.password = null\n",
      "\tssl.truststore.type = JKS\n",
      "\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\n",
      "\ttransaction.max.timeout.ms = 900000\n",
      "\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\n",
      "\ttransaction.state.log.load.buffer.size = 5242880\n",
      "\ttransaction.state.log.min.isr = 1\n",
      "\ttransaction.state.log.num.partitions = 50\n",
      "\ttransaction.state.log.replication.factor = 1\n",
      "\ttransaction.state.log.segment.bytes = 104857600\n",
      "\ttransactional.id.expiration.ms = 604800000\n",
      "\tunclean.leader.election.enable = false\n",
      "\tzookeeper.clientCnxnSocket = null\n",
      "\tzookeeper.connect = localhost:2181\n",
      "\tzookeeper.connection.timeout.ms = 18000\n",
      "\tzookeeper.max.in.flight.requests = 10\n",
      "\tzookeeper.session.timeout.ms = 18000\n",
      "\tzookeeper.set.acl = false\n",
      "\tzookeeper.ssl.cipher.suites = null\n",
      "\tzookeeper.ssl.client.enable = false\n",
      "\tzookeeper.ssl.crl.enable = false\n",
      "\tzookeeper.ssl.enabled.protocols = null\n",
      "\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\n",
      "\tzookeeper.ssl.keystore.location = null\n",
      "\tzookeeper.ssl.keystore.password = null\n",
      "\tzookeeper.ssl.keystore.type = null\n",
      "\tzookeeper.ssl.ocsp.enable = false\n",
      "\tzookeeper.ssl.protocol = TLSv1.2\n",
      "\tzookeeper.ssl.truststore.location = null\n",
      "\tzookeeper.ssl.truststore.password = null\n",
      "\tzookeeper.ssl.truststore.type = null\n",
      "\tzookeeper.sync.time.ms = 2000\n",
      " (kafka.server.KafkaConfig)\n",
      "[2024-11-14 16:38:55,224] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2024-11-14 16:38:55,225] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2024-11-14 16:38:55,276] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2024-11-14 16:38:55,371] INFO Log directory /tmp/kafka-logs not found, creating it. (kafka.log.LogManager)\n",
      "[2024-11-14 16:38:55,407] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)\n",
      "[2024-11-14 16:38:55,411] INFO Attempting recovery for all logs in /tmp/kafka-logs since no clean shutdown file was found (kafka.log.LogManager)\n",
      "[2024-11-14 16:38:55,428] INFO Loaded 0 logs in 19ms. (kafka.log.LogManager)\n",
      "[2024-11-14 16:38:55,458] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)\n",
      "[2024-11-14 16:38:55,474] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)\n",
      "[2024-11-14 16:38:56,572] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)\n",
      "[2024-11-14 16:38:56,623] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)\n",
      "[2024-11-14 16:38:56,653] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2024-11-14 16:38:56,655] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2024-11-14 16:38:56,656] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2024-11-14 16:38:56,664] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2024-11-14 16:38:56,679] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)\n",
      "[2024-11-14 16:38:56,741] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)\n",
      "[2024-11-14 16:38:56,803] INFO Stat of the created znode at /brokers/ids/0 is: 24,24,1731602336797,1731602336797,1,0,0,72057616398155776,188,0,24\n",
      " (kafka.zk.KafkaZkClient)\n",
      "[2024-11-14 16:38:56,804] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:9092, czxid (broker epoch): 24 (kafka.zk.KafkaZkClient)\n",
      "[2024-11-14 16:38:56,911] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2024-11-14 16:38:56,922] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)\n",
      "[2024-11-14 16:38:56,924] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2024-11-14 16:38:56,945] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2024-11-14 16:38:56,983] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)\n",
      "[2024-11-14 16:38:56,996] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)\n",
      "[2024-11-14 16:38:57,009] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1 (kafka.coordinator.transaction.ProducerIdManager)\n",
      "[2024-11-14 16:38:57,013] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 15 milliseconds. (kafka.coordinator.group.GroupMetadataManager)\n",
      "[2024-11-14 16:38:57,034] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)\n",
      "[2024-11-14 16:38:57,048] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n",
      "[2024-11-14 16:38:57,048] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)\n",
      "[2024-11-14 16:38:57,088] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2024-11-14 16:38:57,166] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n",
      "[2024-11-14 16:38:57,176] INFO [SocketServer brokerId=0] Starting socket server acceptors and processors (kafka.network.SocketServer)\n",
      "[2024-11-14 16:38:57,198] INFO [SocketServer brokerId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)\n",
      "[2024-11-14 16:38:57,199] INFO [SocketServer brokerId=0] Started socket server acceptors and processors (kafka.network.SocketServer)\n",
      "[2024-11-14 16:38:57,205] INFO Kafka version: 2.6.0 (org.apache.kafka.common.utils.AppInfoParser)\n",
      "[2024-11-14 16:38:57,205] INFO Kafka commitId: 62abe01bee039651 (org.apache.kafka.common.utils.AppInfoParser)\n",
      "[2024-11-14 16:38:57,205] INFO Kafka startTimeMs: 1731602337199 (org.apache.kafka.common.utils.AppInfoParser)\n",
      "[2024-11-14 16:38:57,206] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-14 16:39:05,898] WARN [SocketServer brokerId=0] Unexpected error from /0:0:0:0:0:0:0:1; closing connection (org.apache.kafka.common.network.Selector)\n",
      "org.apache.kafka.common.network.InvalidReceiveException: Invalid receive (size = 1195725856 larger than 104857600)\n",
      "\tat org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:105)\n",
      "\tat org.apache.kafka.common.network.KafkaChannel.receive(KafkaChannel.java:447)\n",
      "\tat org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:397)\n",
      "\tat org.apache.kafka.common.network.Selector.attemptRead(Selector.java:678)\n",
      "\tat org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:580)\n",
      "\tat org.apache.kafka.common.network.Selector.poll(Selector.java:485)\n",
      "\tat kafka.network.Processor.poll(SocketServer.scala:913)\n",
      "\tat kafka.network.Processor.run(SocketServer.scala:816)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "[2024-11-14 16:39:05,916] WARN [SocketServer brokerId=0] Unexpected error from /0:0:0:0:0:0:0:1; closing connection (org.apache.kafka.common.network.Selector)\n",
      "org.apache.kafka.common.network.InvalidReceiveException: Invalid receive (size = 1195725856 larger than 104857600)\n",
      "\tat org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:105)\n",
      "\tat org.apache.kafka.common.network.KafkaChannel.receive(KafkaChannel.java:447)\n",
      "\tat org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:397)\n",
      "\tat org.apache.kafka.common.network.Selector.attemptRead(Selector.java:678)\n",
      "\tat org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:580)\n",
      "\tat org.apache.kafka.common.network.Selector.poll(Selector.java:485)\n",
      "\tat kafka.network.Processor.poll(SocketServer.scala:913)\n",
      "\tat kafka.network.Processor.run(SocketServer.scala:816)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "[2024-11-14 16:39:05,936] WARN [SocketServer brokerId=0] Unexpected error from /0:0:0:0:0:0:0:1; closing connection (org.apache.kafka.common.network.Selector)\n",
      "org.apache.kafka.common.network.InvalidReceiveException: Invalid receive (size = 1195725856 larger than 104857600)\n",
      "\tat org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:105)\n",
      "\tat org.apache.kafka.common.network.KafkaChannel.receive(KafkaChannel.java:447)\n",
      "\tat org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:397)\n",
      "\tat org.apache.kafka.common.network.Selector.attemptRead(Selector.java:678)\n",
      "\tat org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:580)\n",
      "\tat org.apache.kafka.common.network.Selector.poll(Selector.java:485)\n",
      "\tat kafka.network.Processor.poll(SocketServer.scala:913)\n",
      "\tat kafka.network.Processor.run(SocketServer.scala:816)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "[2024-11-14 16:39:05,947] WARN [SocketServer brokerId=0] Unexpected error from /0:0:0:0:0:0:0:1; closing connection (org.apache.kafka.common.network.Selector)\n",
      "org.apache.kafka.common.network.InvalidReceiveException: Invalid receive (size = 1195725856 larger than 104857600)\n",
      "\tat org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:105)\n",
      "\tat org.apache.kafka.common.network.KafkaChannel.receive(KafkaChannel.java:447)\n",
      "\tat org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:397)\n",
      "\tat org.apache.kafka.common.network.Selector.attemptRead(Selector.java:678)\n",
      "\tat org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:580)\n",
      "\tat org.apache.kafka.common.network.Selector.poll(Selector.java:485)\n",
      "\tat kafka.network.Processor.poll(SocketServer.scala:913)\n",
      "\tat kafka.network.Processor.run(SocketServer.scala:816)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "[2024-11-14 16:40:28,186] INFO Creating topic tp-meteo with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)\n",
      "[2024-11-14 16:40:28,203] INFO [KafkaApi-0] Auto creation of topic tp-meteo with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)\n",
      "[2024-11-14 16:40:28,270] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(tp-meteo-0) (kafka.server.ReplicaFetcherManager)\n",
      "[2024-11-14 16:40:28,332] INFO [Log partition=tp-meteo-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)\n",
      "[2024-11-14 16:40:28,339] INFO Created log for partition tp-meteo-0 in /tmp/kafka-logs/tp-meteo-0 with properties {compression.type -> producer, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, flush.messages -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)\n",
      "[2024-11-14 16:40:28,340] INFO [Partition tp-meteo-0 broker=0] No checkpointed highwatermark is found for partition tp-meteo-0 (kafka.cluster.Partition)\n",
      "[2024-11-14 16:40:28,341] INFO [Partition tp-meteo-0 broker=0] Log loaded for partition tp-meteo-0 with initial high watermark 0 (kafka.cluster.Partition)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Paths to Zookeeper and Kafka start scripts and configuration files\n",
    "ZOOKEEPER_START_CMD = \"./kafka_2.12-2.6.0/bin/zookeeper-server-start.sh\"\n",
    "ZOOKEEPER_CONFIG = \"./kafka_2.12-2.6.0/config/zookeeper.properties\"\n",
    "KAFKA_START_CMD = \"./kafka_2.12-2.6.0/bin/kafka-server-start.sh\"\n",
    "KAFKA_CONFIG = \"./kafka_2.12-2.6.0/config/server.properties\"\n",
    "\n",
    "# Start Zookeeper\n",
    "zookeeper_process = subprocess.Popen([ZOOKEEPER_START_CMD, ZOOKEEPER_CONFIG])\n",
    "print(\"Starting Zookeeper...\")\n",
    "time.sleep(5)  # Wait for Zookeeper to initialize\n",
    "\n",
    "# Start Kafka\n",
    "kafka_process = subprocess.Popen([KAFKA_START_CMD, KAFKA_CONFIG])\n",
    "print(\"Starting Kafka...\")\n",
    "time.sleep(5)  # Wait for Kafka to initialize\n",
    "\n",
    "# Kafka and Zookeeper are now running in the background.\n",
    "# You can proceed with other operations here, like producing and consuming messages.\n",
    "\n",
    "# To stop the servers after usage\n",
    "def stop_servers():\n",
    "    zookeeper_process.terminate()\n",
    "    kafka_process.terminate()\n",
    "    print(\"Zookeeper and Kafka have been stopped.\")\n",
    "\n",
    "# Example usage of stopping the servers\n",
    "# stop_servers()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sudo apt install python3.10-venv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kafka.producer.future.FutureRecordMetadata at 0x73fa1029a2c0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemple pour collecter des données dans un topic Kafka\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "\n",
    "# Configuration\n",
    "KAFKA_SERVER = 'localhost:9092'\n",
    "\n",
    "# Initialisation du producteur Kafka\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[KAFKA_SERVER],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "data =  {'temp': 11.32,\n",
    "  'feels_like': 10.67,\n",
    "  'temp_min': 10.21,\n",
    "  'temp_max': 12.36,\n",
    "  'pressure': 1030,\n",
    "  'humidity': 83,\n",
    "  'sea_level': 1030,\n",
    "  'grnd_level': 1020}\n",
    "\n",
    "# Envoi des données en continu\n",
    "\n",
    "producer.send(\"tp-meteo\",  value=data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "# Configuration de Kafka\n",
    "KAFKA_TOPIC = 'tp-meteo'\n",
    "KAFKA_SERVER = 'localhost:9092'\n",
    "\n",
    "# Initialisation du consumer Kafka\n",
    "consumer = KafkaConsumer(\n",
    "    KAFKA_TOPIC,\n",
    "    bootstrap_servers=[KAFKA_SERVER],\n",
    "    auto_offset_reset='earliest',\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    ")\n",
    "\n",
    "# Consommer et afficher les messages\n",
    "for message in consumer:\n",
    "    data = message.value       \n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP : Pipeline de Données Météorologiques en Temps Réel avec Kafka\n",
    "\n",
    "**Objectif** : Dans ce TP, vous allez utiliser Kafka pour transmettre des données météorologiques en temps réel d’un producteur (qui récupère les données) vers un consumer (qui les affiche). Vous configurerez un producteur pour récupérer les données météo de plusieurs villes via une API, puis un consumer pour consommer et afficher ces données en continu.\n",
    "\n",
    "---\n",
    "\n",
    "## Partie 1 : Producteur Kafka – Récupération et Envoi des Données Météorologiques\n",
    "\n",
    "Le producteur Kafka va récupérer les données météo pour trois villes cibles et les envoyer dans un topic Kafka nommé `tp-meteo`. \n",
    "\n",
    "### Étapes :\n",
    "\n",
    "1. **Configuration de l'API et des Villes** :\n",
    "   - Utilisez une clé API pour accéder à OpenWeatherMap (une API de données météo).\n",
    "   - Définissez trois villes cibles : `Paris`, `London`, et `Tokyo`.\n",
    "\n",
    "2. **Initialisation du Producteur Kafka** :\n",
    "   - Configurez Kafka pour qu’il envoie des messages au serveur `localhost:9092` dans le topic `tp-meteo`.\n",
    "   - Kafka sera utilisé pour envoyer les données météorologiques formatées en JSON.\n",
    "\n",
    "3. **Fonction de Récupération des Données** :\n",
    "   - Écrivez une fonction `get_weather_data` qui récupère les informations météo pour une ville donnée.\n",
    "   - La fonction envoie une requête à l’API et retourne les données si la requête est réussie.\n",
    "\n",
    "4. **Boucle d’Envoi en Continu** :\n",
    "   - Utilisez une boucle pour envoyer les données de chaque ville au topic Kafka toutes les minutes.\n",
    "   - Affichez un message de confirmation dans la console pour chaque envoi réussi.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from kafka import KafkaProducer, KafkaConsumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de l'API et Kafka\n",
    "API_KEY = '5cd0c68e3773c0513b41138ade44163b'  # Remplacez par votre clé API\n",
    "CITIES = ['Paris', 'London', 'Tokyo']  # Villes cibles\n",
    "KAFKA_SERVER = 'localhost:9092'  # Serveur Kafka\n",
    "KAFKA_TOPIC = 'tp-meteo'  # Nom du topic Kafka\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producteur Kafka initialisé.\n"
     ]
    }
   ],
   "source": [
    "# Initialisation du producteur Kafka\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[KAFKA_SERVER],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "print(\"Producteur Kafka initialisé.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour récupérer les données météo d'une ville\n",
    "def get_weather_data(city):\n",
    "    url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={API_KEY}&units=metric\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Échec de la récupération des données pour {city}, statut : {response.status_code}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données météo envoyées pour Paris dans le topic Kafka.\n",
      "Données météo envoyées pour London dans le topic Kafka.\n",
      "Données météo envoyées pour Tokyo dans le topic Kafka.\n",
      "Données météo envoyées pour Paris dans le topic Kafka.\n",
      "Données météo envoyées pour London dans le topic Kafka.\n",
      "Données météo envoyées pour Tokyo dans le topic Kafka.\n",
      "Données météo envoyées pour Paris dans le topic Kafka.\n",
      "Données météo envoyées pour London dans le topic Kafka.\n",
      "Données météo envoyées pour Tokyo dans le topic Kafka.\n",
      "Données météo envoyées pour Paris dans le topic Kafka.\n",
      "Données météo envoyées pour London dans le topic Kafka.\n",
      "Données météo envoyées pour Tokyo dans le topic Kafka.\n",
      "Données météo envoyées pour Paris dans le topic Kafka.\n",
      "Données météo envoyées pour London dans le topic Kafka.\n",
      "Données météo envoyées pour Tokyo dans le topic Kafka.\n",
      "Données météo envoyées pour Paris dans le topic Kafka.\n",
      "Données météo envoyées pour London dans le topic Kafka.\n",
      "Données météo envoyées pour Tokyo dans le topic Kafka.\n",
      "Données météo envoyées pour Paris dans le topic Kafka.\n",
      "Données météo envoyées pour London dans le topic Kafka.\n",
      "Données météo envoyées pour Tokyo dans le topic Kafka.\n",
      "Données météo envoyées pour Paris dans le topic Kafka.\n",
      "Données météo envoyées pour London dans le topic Kafka.\n",
      "Données météo envoyées pour Tokyo dans le topic Kafka.\n",
      "Données météo envoyées pour Paris dans le topic Kafka.\n",
      "Données météo envoyées pour London dans le topic Kafka.\n",
      "Données météo envoyées pour Tokyo dans le topic Kafka.\n",
      "Données météo envoyées pour Paris dans le topic Kafka.\n",
      "Données météo envoyées pour London dans le topic Kafka.\n",
      "Données météo envoyées pour Tokyo dans le topic Kafka.\n"
     ]
    }
   ],
   "source": [
    "# Boucle d'envoi en continu des données météo au topic Kafka\n",
    "while True:\n",
    "    for city in CITIES:\n",
    "        data = get_weather_data(city)\n",
    "        if data:\n",
    "            # Extraction des données météo pertinentes\n",
    "            weather_info = {\n",
    "                'city': city,\n",
    "                'temp': data['main']['temp'],\n",
    "                'feels_like': data['main']['feels_like'],\n",
    "                'temp_min': data['main']['temp_min'],\n",
    "                'temp_max': data['main']['temp_max'],\n",
    "                'pressure': data['main']['pressure'],\n",
    "                'humidity': data['main']['humidity']\n",
    "            }\n",
    "            # Envoi des données dans le topic Kafka\n",
    "            producer.send(KAFKA_TOPIC, value=weather_info)\n",
    "            print(f\"Données météo envoyées pour {city} dans le topic Kafka.\")\n",
    "    # Attente de 60 secondes avant de récupérer à nouveau les données\n",
    "    time.sleep(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumer Kafka initialisé et prêt à recevoir les données.\n"
     ]
    }
   ],
   "source": [
    "# Initialisation du consumer Kafka\n",
    "consumer = KafkaConsumer(\n",
    "    KAFKA_TOPIC,\n",
    "    bootstrap_servers=[KAFKA_SERVER],\n",
    "    auto_offset_reset='earliest',\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    ")\n",
    "print(\"Consumer Kafka initialisé et prêt à recevoir les données.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kafka_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
